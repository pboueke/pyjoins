{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyjoins\n",
    "\n",
    "This notebook aims to compare a few join algorithms using simple in-memory index tables and disk I/O with python 2.7\n",
    "\n",
    "For this purposes, we are using simple CSV-like files containing the data that will be used for our joins. For indexing, we are using the default python dictionary for our hash index and the OOBtree from the BTrees module for our btree index. Also, the data used in our tests is generated by a generator script at the project [repository](https://github.com/pboueke/pyjoins).\n",
    "\n",
    "We will be testing nested loop joins, merge-joins, hash-joins and indexed joins. We will be testing for files (ordered and unordered) indexed with both the hash index and the btree index.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BTrees.OOBTree import OOBTree\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And generating the data\n",
    "\n",
    "Using the generator.py script (at the repository), we will generate 4 files:\n",
    "\n",
    "1. data_ordered_primary.dat\n",
    "2. data_ordered_secondary.dat\n",
    "3. data_unordered_primary.dat\n",
    "4. data_unordered_secondary.dat\n",
    "\n",
    "Parameters used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 100000\n",
    "field_delimiter = \"|\"\n",
    "record_size =2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in two datasets, called \"primary\" and \"secondary\". The primary contains two columns, one for its key and another with random characters. The secondary three columnes: its primary key, its foreign key (the primary key from the related record of the primary dataset) and a random charachter string. All registers contain 2048 characters and each line 2049, as defined above. Each dataset is generated in two versions, one ordered and another unordered, where the ordering is set by the ascending order of the primary key of the primary dataset. There is one register by line, a  \"size\" number of lines, and all relations between the two datasets are 1 to 1.\n",
    "\n",
    "### Creating indexes\n",
    "\n",
    "For simplicity, we shall create two index tables: one for each index type and for the unordered secondary dataset. Note that we are not considering the sizes, as, in this exercise, both datasets have the same size and we are always starting our scans at the primary one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash table size: 100000\n",
      "Btree size: 100000\n"
     ]
    }
   ],
   "source": [
    "# instantiate our tables\n",
    "# for hashtables, we are using the default python dictionary\n",
    "hash_table = {}\n",
    "# for the btree, we are using the BTrees package\n",
    "btree = OOBTree() \n",
    "\n",
    "line_counter = 0\n",
    "with open (\"data_unordered_secondary.dat\") as f:\n",
    "    for line in f:\n",
    "        # \"primary_key|foreign_key|data\"\n",
    "        l = line.split(field_delimiter)\n",
    "        hash_table[l[1]] = line_counter\n",
    "        btree.update({l[1]:line_counter})\n",
    "        line_counter += 1\n",
    "\n",
    "# checking the size\n",
    "print \"Hash table size: \" + str(len(hash_table))\n",
    "print \"Btree size: \" + str(len(btree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting with a simple Nested Loop\n",
    "\n",
    "We don't need the indexes here. We will simply scan the ordered primary dataset file, line by line, and, for each line, we will scan the unordered secondary dataset until we find our match. As this is just a comparison exercise, we will just compute the join and track the time, not keeping the results in memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total joined registers: 100000\n",
      "Time taken: 17124.955875s\n"
     ]
    }
   ],
   "source": [
    "start = dt.datetime.now()\n",
    "matches = 0\n",
    "with open(\"data_unordered_secondary.dat\") as right:\n",
    "    with open(\"data_ordered_primary.dat\") as left:\n",
    "        for line1 in left:\n",
    "            l1 = line1.split(field_delimiter)\n",
    "            for line2 in right:\n",
    "                l2 = line2.split(field_delimiter)\n",
    "                if l1[0] == l2[1]:\n",
    "                    joined = l1 + l2\n",
    "                    matches += 1\n",
    "                    right.seek(0)\n",
    "                    break\n",
    "end = dt.datetime.now()                    \n",
    "\n",
    "print \"Total joined registers: \" + str(matches)\n",
    "print \"Time taken: \" + str((end-start).total_seconds()) + \"s\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Join\n",
    "\n",
    "A sort-merge join approach should yield much faster results, but it requires both inputs to be ordered. As we are simulating an enviroment where we could not simply load all the data into the main memory, we must assume that both files are ordered, and so we shall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total joined registers: 100000\n",
      "Time taken: 2.272333s\n"
     ]
    }
   ],
   "source": [
    "start = dt.datetime.now()\n",
    "matches = 0\n",
    "with open(\"data_ordered_secondary.dat\") as right:\n",
    "    with open(\"data_ordered_primary.dat\") as left:\n",
    "        left_rec = left.readline().split(field_delimiter)\n",
    "        right_rec = right.readline().split(field_delimiter)\n",
    "        while True:\n",
    "            if left_rec[0] == right_rec[1]:\n",
    "                matches += 1\n",
    "                joined = left_rec + right_rec\n",
    "            if int(left_rec[0]) < int(right_rec[1]):\n",
    "                ln = left.readline()\n",
    "                if ln == \"\":\n",
    "                    break\n",
    "                left_rec = ln.split(field_delimiter)\n",
    "            else: \n",
    "                ln = right.readline()\n",
    "                if ln == \"\":\n",
    "                    break\n",
    "                right_rec = ln.split(field_delimiter)\n",
    "end = dt.datetime.now()   \n",
    "    \n",
    "print \"Total joined registers: \" + str(matches)\n",
    "print \"Time taken: \" + str((end-start).total_seconds()) + \"s\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hash Join\n",
    "\n",
    "The hash join is similar to using our hash table index, but we must account the time required to create the hash table, since we cannot take for granted its existence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total joined registers: 100000\n",
      "Time taken: 1.423748s\n"
     ]
    }
   ],
   "source": [
    "line_size = record_size + 1\n",
    "start = dt.datetime.now()\n",
    "\n",
    "temp_hash_table = {}\n",
    "\n",
    "line_counter = 0\n",
    "with open (\"data_unordered_secondary.dat\") as f:\n",
    "    for line in f:\n",
    "        # \"primary_key|foreign_key|data\"\n",
    "        l = line.split(field_delimiter)\n",
    "        temp_hash_table[l[1]] = line_counter\n",
    "        line_counter += 1\n",
    "        \n",
    "matches = 0\n",
    "with open(\"data_unordered_secondary.dat\") as right:\n",
    "    with open(\"data_unordered_primary.dat\") as left:\n",
    "        for line in left:\n",
    "            left_rec = line.split(field_delimiter)\n",
    "            position = temp_hash_table[left_rec[0]] * line_size\n",
    "            right.seek(position)\n",
    "            right_rec = right.read(record_size).split(field_delimiter)\n",
    "            if left_rec[0] == right_rec[1]:\n",
    "                matches += 1\n",
    "                joined = left_rec + right_rec\n",
    "            else:\n",
    "                print \"This should never happen!\"\n",
    "end = dt.datetime.now()   \n",
    "    \n",
    "print \"Total joined registers: \" + str(matches)\n",
    "print \"Time taken: \" + str((end-start).total_seconds()) + \"s\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join using the Hashtable Index\n",
    "\n",
    "Basically the same as the method above, but we are finnaly we will be using one of our indexes. We will scan the unordered primary data (wouldn't make much of a difference if we used the ordered) and will use the hash table to look for the data at the unordered secondary data table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total joined registers: 100000\n",
      "Time taken: 1.036432s\n"
     ]
    }
   ],
   "source": [
    "line_size = record_size + 1\n",
    "start = dt.datetime.now()\n",
    "matches = 0\n",
    "with open(\"data_unordered_secondary.dat\") as right:\n",
    "    with open(\"data_unordered_primary.dat\") as left:\n",
    "        for line in left:\n",
    "            left_rec = line.split(field_delimiter)\n",
    "            position = hash_table[left_rec[0]] * line_size\n",
    "            right.seek(position)\n",
    "            right_rec = right.read(record_size).split(field_delimiter)\n",
    "            if left_rec[0] == right_rec[1]:\n",
    "                matches += 1\n",
    "                joined = left_rec + right_rec\n",
    "            else:\n",
    "                print \"This should never happen!\"\n",
    "end = dt.datetime.now()   \n",
    "    \n",
    "print \"Total joined registers: \" + str(matches)\n",
    "print \"Time taken: \" + str((end-start).total_seconds()) + \"s\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join using the BTree index\n",
    "\n",
    "Same as the above, bu using the btree index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total joined registers: 100000\n",
      "Time taken: 1.230882s\n"
     ]
    }
   ],
   "source": [
    "line_size = record_size + 1\n",
    "start = dt.datetime.now()\n",
    "matches = 0\n",
    "with open(\"data_unordered_secondary.dat\") as right:\n",
    "    with open(\"data_unordered_primary.dat\") as left:\n",
    "        for line in left:\n",
    "            left_rec = line.split(field_delimiter)\n",
    "            position = btree[left_rec[0]] * line_size\n",
    "            right.seek(position)\n",
    "            right_rec = right.read(record_size).split(field_delimiter)\n",
    "            if left_rec[0] == right_rec[1]:\n",
    "                matches += 1\n",
    "                joined = left_rec + right_rec\n",
    "            else:\n",
    "                print \"This should never happen!\"\n",
    "end = dt.datetime.now()   \n",
    "    \n",
    "print \"Total joined registers: \" + str(matches)\n",
    "print \"Time taken: \" + str((end-start).total_seconds()) + \"s\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
