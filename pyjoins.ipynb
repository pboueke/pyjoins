{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pyjoins\n",
    "\n",
    "This notebook aims to compare a few join algorithms using simple in-memory index tables and disk I/O with python 2.7\n",
    "\n",
    "For this purposes, we are using simple CSV-like files containing the data that will be used for our joins. For indexing, we are using the default python dictionary for our hash index and the OOBtree from the BTrees module for our btree index. Also, the data used our tests is generated by a generator script at the project [repository](https://github.com/pboueke/pyjoins).\n",
    "\n",
    "We will be testing nested loop joins, merge-joins and hash-joins. We will be testing for files (ordered and unordered) indexed with both the hash index and the btree index.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BTrees.OOBTree import OOBTree\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And generating the data\n",
    "\n",
    "Using the generator.py script (at the repository), we will generate 4 files:\n",
    "\n",
    "1. data_ordered_primary.dat\n",
    "2. data_ordered_secondary.dat\n",
    "3. data_unordered_primary.dat\n",
    "4. data_unordered_secondary.dat\n",
    "\n",
    "Parameters used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 100000\n",
    "field_delimiter = \"|\"\n",
    "record_size =2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This results in two datasets, called \"primary\" and \"secondary\". The primary contains two columns, one for its key and another with random characters. The secondary three columnes: its primary key, its foreign key (the primary key from the related record of the primary dataset) and a random charachter string. All registers contain 2048 characters and each line 2049, as defined above. Each dataset is generated in two versions, one ordered and another unordered, where the ordering is set by the ascending order of the primary key of the primary dataset. There is one register by line, a  \"size\" number of lines, and all relations between the two datasets are 1 to 1.\n",
    "\n",
    "### Creating indexes\n",
    "\n",
    "For simplicity, we shall create two index tables: one for each index type and for the unordered secondary dataset. Note that we are not considering the sizes, as, in this exercise, both datasets have the same size and we are always starting our scans at the primary one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash table size: 10000\n",
      "Btree size: 10000\n"
     ]
    }
   ],
   "source": [
    "# instantiate our tables\n",
    "# for hashtables, we are using the default python dictionary\n",
    "hash_table = {}\n",
    "# for the btree, we are using the BTrees package\n",
    "btree = OOBTree() \n",
    "\n",
    "line_counter = 0\n",
    "with open (\"data_unordered_secondary.dat\") as f:\n",
    "    for line in f:\n",
    "        # \"primary_key|foreign_key|data\"\n",
    "        l = line.split(field_delimiter)\n",
    "        hash_table[l[1]] = line_counter\n",
    "        btree.update({l[1]:line_counter})\n",
    "        line_counter += 1\n",
    "\n",
    "# checking the size\n",
    "print \"Hash table size: \" + str(len(hash_table))\n",
    "print \"Btree size: \" + str(len(btree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting with a simple nested loop\n",
    "\n",
    "We don't need the indexes here. We will simply scan the ordered primary dataset file, line by line, and, for each line, we will scan the unordered secondary dataset until we find our match. As this is just a comparison exercise, we will just compute the join and track the time, not keeping the results in memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total joined registers: 10000\n",
      "Time taken: 180.545651s\n"
     ]
    }
   ],
   "source": [
    "start = dt.datetime.now()\n",
    "matches = 0\n",
    "right = open(\"data_unordered_secondary.dat\")\n",
    "with open(\"data_ordered_primary.dat\") as left:\n",
    "    for line1 in left:\n",
    "        l1 = line1.split(field_delimiter)\n",
    "        for line2 in right:\n",
    "            l2 = line2.split(field_delimiter)\n",
    "            if l1[0] == l2[1]:\n",
    "                joined = l1 + l2\n",
    "                matches += 1\n",
    "                right.seek(0)\n",
    "                break\n",
    "right.close()\n",
    "end = dt.datetime.now()                    \n",
    "\n",
    "print \"Total joined registers: \" + str(matches)\n",
    "print \"Time taken: \" + str((end-start).total_seconds()) + \"s\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge-join\n",
    "\n",
    "A sort-merge join approach should yield much faster results, but it requires both inputs to be ordered. As we are simulating an enviroment where we could not simply load all the data into the main memory, we must"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
